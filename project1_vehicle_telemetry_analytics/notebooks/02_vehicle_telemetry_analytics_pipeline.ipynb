{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3458971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and environment setup\n",
    "# -----------------------------------\n",
    "# Purpose:\n",
    "# - Centralize imports\n",
    "# - Define project directory structure\n",
    "# - Ensure repeatable execution\n",
    "# - Keep paths generic and public-safe\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Project root (generic)\n",
    "PROJECT_ROOT = Path(\n",
    "    r\"YOUR FOLDER PATH\"\n",
    ")\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw_telemetry_csv\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed_telemetry\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "for d in [RAW_DIR, PROCESSED_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Python version:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Ingest decoded telemetry files\n",
    "# ------------------------------------\n",
    "# Purpose:\n",
    "# - Treat incoming CSVs as decoded telemetry outputs\n",
    "# - Preserve raw data for traceability\n",
    "# - Keep ingestion logic independent of analytics\n",
    "\n",
    "INCOMING_DIR = DATA_DIR / \"incoming_telemetry\"\n",
    "csv_files = list(INCOMING_DIR.glob(\"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    raise RuntimeError(\"No telemetry CSV files found in incoming_telemetry\")\n",
    "\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    df.to_csv(RAW_DIR / f.name, index=False)\n",
    "\n",
    "print(f\"Ingested {len(csv_files)} telemetry files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a431c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Processing & analytics-ready signal retention\n",
    "# ----------------------------------------------------\n",
    "# Purpose:\n",
    "# - Clean telemetry signals\n",
    "# - Enforce physical plausibility\n",
    "# - Retain only analytics-relevant channels\n",
    "# - Produce normalized telemetry for downstream analytics\n",
    "\n",
    "def process_trip(df, source_file):\n",
    "    df = df.copy()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Timestamp handling\n",
    "    # --------------------------------------------------\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Basic plausibility filtering\n",
    "    # --------------------------------------------------\n",
    "    if \"engine_rpm\" in df:\n",
    "        df.loc[df[\"engine_rpm\"] < 500, \"engine_rpm\"] = np.nan\n",
    "        df.loc[df[\"engine_rpm\"] > 6000, \"engine_rpm\"] = np.nan\n",
    "\n",
    "    if \"torque_nm\" in df:\n",
    "        df.loc[df[\"torque_nm\"] < -180, \"torque_nm\"] = np.nan\n",
    "        df.loc[df[\"torque_nm\"] > 200, \"torque_nm\"] = np.nan\n",
    "\n",
    "    if \"speed_kmph\" in df:\n",
    "        df.loc[df[\"speed_kmph\"] < 0, \"speed_kmph\"] = np.nan\n",
    "        df.loc[df[\"speed_kmph\"] > 140, \"speed_kmph\"] = np.nan\n",
    "\n",
    "    # Short-gap fill\n",
    "    df = df.ffill(limit=5)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Mandatory identifiers\n",
    "    # --------------------------------------------------\n",
    "    if \"vehicle_id\" not in df:\n",
    "        df[\"vehicle_id\"] = source_file.split(\"_\")[0]\n",
    "\n",
    "    if \"scenario\" not in df:\n",
    "        df[\"scenario\"] = \"unknown\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Distance integration (meters)\n",
    "    # --------------------------------------------------\n",
    "    if \"distance_m\" not in df and \"speed_kmph\" in df:\n",
    "        dt = df[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "        df[\"distance_m\"] = (df[\"speed_kmph\"] * 1000 / 3600 * dt).cumsum()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Analytics-ready signal set (generic)\n",
    "    # --------------------------------------------------\n",
    "    REQUIRED_SIGNAL_SET = [\n",
    "        \"timestamp\",\n",
    "        \"vehicle_id\",\n",
    "        \"scenario\",\n",
    "\n",
    "        \"engine_rpm\",\n",
    "        \"torque_nm\",\n",
    "        \"speed_kmph\",\n",
    "        \"grade_pct\",\n",
    "        \"distance_m\",\n",
    "\n",
    "        \"oil_temp_c\",\n",
    "        \"coolant_temp_c\",\n",
    "\n",
    "        \"current_gear\",\n",
    "        \"selected_gear\",\n",
    "        \"clutch_state\",\n",
    "\n",
    "        \"vibration_ax_g\"\n",
    "    ]\n",
    "\n",
    "    df = df[[c for c in REQUIRED_SIGNAL_SET if c in df.columns]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Batch processing\n",
    "count = 0\n",
    "for f in RAW_DIR.glob(\"*.csv\"):\n",
    "    df_raw = pd.read_csv(f)\n",
    "    df_proc = process_trip(df_raw, f.name)\n",
    "\n",
    "    df_proc.to_csv(PROCESSED_DIR / f.name, index=False)\n",
    "    count += 1\n",
    "\n",
    "print(f\"Processed telemetry files: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper function to load processed telemetry\n",
    "# --------------------------------------------------\n",
    "# Purpose:\n",
    "# - Centralize CSV loading\n",
    "# - Enforce timestamp ordering\n",
    "\n",
    "def load_processed_csv(path):\n",
    "    df = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
    "    return df.sort_values(\"timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Operating profile analytics\n",
    "# ------------------------------------------------\n",
    "# Purpose:\n",
    "# - Quantify time spent in observed operating regions\n",
    "# - Avoid cartesian explosion\n",
    "# - Produce BI-ready bin indices and labels\n",
    "\n",
    "RPM_BINS = np.arange(800, 6001, 250)\n",
    "TORQUE_BINS = np.arange(-180, 201, 25)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for f in PROCESSED_DIR.glob(\"*.csv\"):\n",
    "    df = load_processed_csv(f)[[\"timestamp\", \"engine_rpm\", \"torque_nm\"]].dropna()\n",
    "\n",
    "    df[\"delta_t\"] = df[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    df[\"rpm_bin_idx\"] = pd.cut(df[\"engine_rpm\"], RPM_BINS, labels=False)\n",
    "    df[\"torque_bin_idx\"] = pd.cut(df[\"torque_nm\"], TORQUE_BINS, labels=False)\n",
    "\n",
    "    df = df.dropna(subset=[\"rpm_bin_idx\", \"torque_bin_idx\"])\n",
    "    df[[\"rpm_bin_idx\", \"torque_bin_idx\"]] = df[[\"rpm_bin_idx\", \"torque_bin_idx\"]].astype(int)\n",
    "\n",
    "    grp = (\n",
    "        df.groupby([\"rpm_bin_idx\", \"torque_bin_idx\"], as_index=False)[\"delta_t\"]\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    grp[\"rpm_bin_low\"] = grp[\"rpm_bin_idx\"].apply(lambda i: RPM_BINS[i])\n",
    "    grp[\"rpm_bin_high\"] = grp[\"rpm_bin_idx\"].apply(lambda i: RPM_BINS[i + 1])\n",
    "    grp[\"rpm_bin_label\"] = grp[\"rpm_bin_low\"].astype(str) + \"–\" + grp[\"rpm_bin_high\"].astype(str)\n",
    "\n",
    "    grp[\"torque_bin_low\"] = grp[\"torque_bin_idx\"].apply(lambda i: TORQUE_BINS[i])\n",
    "    grp[\"torque_bin_high\"] = grp[\"torque_bin_idx\"].apply(lambda i: TORQUE_BINS[i + 1])\n",
    "    grp[\"torque_bin_label\"] = grp[\"torque_bin_low\"].astype(str) + \"–\" + grp[\"torque_bin_high\"].astype(str)\n",
    "\n",
    "    grp[\"source_file\"] = f.name\n",
    "    rows.append(grp)\n",
    "\n",
    "operating_profile_df = pd.concat(rows, ignore_index=True)\n",
    "operating_profile_df.to_csv(OUTPUT_DIR / \"telemetry_operating_profile.csv\", index=False)\n",
    "\n",
    "print(\"Operating profile analytics generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Thermal behavior analytics\n",
    "# --------------------------------\n",
    "# Purpose:\n",
    "# - Monitor thermal signals\n",
    "# - Quantify excursions beyond safe operating ranges\n",
    "\n",
    "rows = []\n",
    "\n",
    "for f in PROCESSED_DIR.glob(\"*.csv\"):\n",
    "    df = load_processed_csv(f)[[\"timestamp\", \"oil_temp_c\", \"coolant_temp_c\"]].dropna()\n",
    "\n",
    "    df[\"oil_over_limit\"] = df[\"oil_temp_c\"] > 110\n",
    "    df[\"coolant_out_of_range\"] = (df[\"coolant_temp_c\"] < 60) | (df[\"coolant_temp_c\"] > 90)\n",
    "\n",
    "    df[\"oil_overshoot_count\"] = df[\"oil_over_limit\"].astype(int)\n",
    "    df[\"coolant_overshoot_count\"] = df[\"coolant_out_of_range\"].astype(int)\n",
    "\n",
    "    df[\"source_file\"] = f.name\n",
    "    rows.append(df)\n",
    "\n",
    "thermal_df = pd.concat(rows, ignore_index=True)\n",
    "thermal_df.to_csv(OUTPUT_DIR / \"telemetry_thermal_behavior.csv\", index=False)\n",
    "\n",
    "print(\"Thermal behavior analytics generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Environmental profile analytics\n",
    "# -------------------------------------\n",
    "# Purpose:\n",
    "# - Analyze distribution of environmental signals (e.g., grade)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for f in PROCESSED_DIR.glob(\"*.csv\"):\n",
    "    df = load_processed_csv(f)\n",
    "\n",
    "    if \"grade_pct\" not in df:\n",
    "        continue\n",
    "\n",
    "    g = df[[\"grade_pct\"]].dropna()\n",
    "    g[\"source_file\"] = f.name\n",
    "    rows.append(g)\n",
    "\n",
    "env_df = pd.concat(rows, ignore_index=True)\n",
    "env_df.to_csv(OUTPUT_DIR / \"telemetry_environmental_profile.csv\", index=False)\n",
    "\n",
    "print(\"Environmental profile analytics generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Actuation and state transition analytics\n",
    "# -----------------------------------------------\n",
    "# Purpose:\n",
    "# - Quantify actuation events\n",
    "# - Detect state mismatches\n",
    "# - Normalize by distance\n",
    "\n",
    "rows = []\n",
    "\n",
    "for f in PROCESSED_DIR.glob(\"*.csv\"):\n",
    "    df = load_processed_csv(f)\n",
    "\n",
    "    required = {\"current_gear\", \"selected_gear\", \"clutch_state\", \"distance_m\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        continue\n",
    "\n",
    "    clutch_diff = df[\"clutch_state\"].diff().fillna(0)\n",
    "\n",
    "    engage = int((clutch_diff > 0).sum())\n",
    "    disengage = int((clutch_diff < 0).sum())\n",
    "    mismatch = int((df[\"current_gear\"] != df[\"selected_gear\"]).sum())\n",
    "\n",
    "    distance_km = df[\"distance_m\"].max() / 1000 if df[\"distance_m\"].max() > 0 else np.nan\n",
    "    events_total = engage + disengage\n",
    "\n",
    "    rows.append({\n",
    "        \"source_file\": f.name,\n",
    "        \"vehicle_id\": df[\"vehicle_id\"].iloc[0],\n",
    "        \"scenario\": df[\"scenario\"].iloc[0],\n",
    "        \"actuation_engage_count\": engage,\n",
    "        \"actuation_disengage_count\": disengage,\n",
    "        \"state_mismatch_count\": mismatch,\n",
    "        \"distance_km\": distance_km,\n",
    "        \"events_per_km\": events_total / distance_km if distance_km else np.nan\n",
    "    })\n",
    "\n",
    "actuation_df = pd.DataFrame(rows)\n",
    "actuation_df.to_csv(OUTPUT_DIR / \"telemetry_actuation_statistics.csv\", index=False)\n",
    "\n",
    "print(\"Actuation statistics generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Frequency-domain analytics\n",
    "# --------------------------------\n",
    "# Purpose:\n",
    "# - Extract vibration frequency features\n",
    "# - Compute total spectral energy\n",
    "# - Identify dominant frequencies\n",
    "\n",
    "rows = []\n",
    "\n",
    "for f in PROCESSED_DIR.glob(\"*.csv\"):\n",
    "    df = load_processed_csv(f)\n",
    "\n",
    "    if \"vibration_ax_g\" not in df:\n",
    "        continue\n",
    "\n",
    "    sig = df[[\"timestamp\", \"vibration_ax_g\"]].dropna()\n",
    "\n",
    "    dt = sig[\"timestamp\"].diff().dt.total_seconds().median()\n",
    "    dt = 1.0 if not dt or dt <= 0 else dt\n",
    "\n",
    "    x = sig[\"vibration_ax_g\"].values\n",
    "    yf = rfft(x)\n",
    "    xf = rfftfreq(len(x), dt)\n",
    "    mag = np.abs(yf)\n",
    "\n",
    "    peaks = np.argsort(mag)[-3:][::-1]\n",
    "\n",
    "    rows.append({\n",
    "        \"source_file\": f.name,\n",
    "        \"fft_energy\": float((mag ** 2).sum()),\n",
    "        \"dominant_frequencies\": str([(float(xf[i]), float(mag[i])) for i in peaks])\n",
    "    })\n",
    "\n",
    "fft_df = pd.DataFrame(rows)\n",
    "fft_df.to_csv(OUTPUT_DIR / \"telemetry_frequency_features.csv\", index=False)\n",
    "\n",
    "print(\"Frequency-domain analytics generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de705e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: MySQL database bootstrap\n",
    "# --------------------------------\n",
    "# Purpose:\n",
    "# - Ensure database exists before uploading tables\n",
    "# - Prevent runtime failures in SQLAlchemy\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"mypassword!\"\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_PORT = \"3306\"\n",
    "MYSQL_DB = \"telemetry_analytics_db\"\n",
    "\n",
    "# Connect WITHOUT database first\n",
    "engine_bootstrap = create_engine(\n",
    "    f\"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}\"\n",
    ")\n",
    "\n",
    "with engine_bootstrap.connect() as conn:\n",
    "    conn.execute(text(f\"CREATE DATABASE IF NOT EXISTS {MYSQL_DB}\"))\n",
    "    print(f\"Database '{MYSQL_DB}' ensured\")\n",
    "\n",
    "# Create engine WITH database\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}\"\n",
    ")\n",
    "\n",
    "print(\"Connected to MySQL database:\", MYSQL_DB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Cell: Upload analytics outputs to MySQL\n",
    "# ---------------------------------------------\n",
    "# Purpose:\n",
    "# - Upload analytics tables\n",
    "# - Enable Power BI live connection & refresh\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def upload_table(csv_file, table_name):\n",
    "    df = pd.read_csv(OUTPUT_DIR / csv_file)\n",
    "    df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "    print(f\"Uploaded {table_name}: {len(df)} rows\")\n",
    "\n",
    "upload_table(\"telemetry_operating_profile.csv\", \"telemetry_operating_profile\")\n",
    "upload_table(\"telemetry_thermal_behavior.csv\", \"telemetry_thermal_behavior\")\n",
    "upload_table(\"telemetry_environmental_profile.csv\", \"telemetry_environmental_profile\")\n",
    "upload_table(\"telemetry_actuation_statistics.csv\", \"telemetry_actuation_statistics\")\n",
    "upload_table(\"telemetry_frequency_features.csv\", \"telemetry_frequency_features\")\n",
    "\n",
    "print(\"All analytics tables uploaded successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
